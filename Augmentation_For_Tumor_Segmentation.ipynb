{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Augmentation_For_Tumor_Segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDYJXahD0B6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connect to google drive on google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP4Ha9IGWiiO",
        "colab_type": "text"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi3YIMiB0eEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import math, random\n",
        "from glob import glob\n",
        "from shutil import copyfile\n",
        "from time import time\n",
        "import imageio, getopt, gzip, cv2  \n",
        "import warnings, pickle\n",
        "import nibabel as nib \n",
        "from nibabel.testing import data_path\n",
        "\n",
        "from IPython.display import Image\n",
        "from PIL import Image\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "warnings.filterwarnings('ignore')\n",
        "from torch.utils.data import Dataset, SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJdovUcWta-",
        "colab_type": "text"
      },
      "source": [
        "# **Sort Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28rxPSaT1SZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define paths and make directories ##############################################\n",
        "bratsPath = '/content/gdrive/My Drive/Brats19'\n",
        "\n",
        "niftiHGGPath = os.path.join(bratsPath, 'MICCAI_BraTS_2019_Data_Training/HGG')\n",
        "niftiLGGPath = os.path.join(bratsPath, 'MICCAI_BraTS_2019_Data_Training/LGG')\n",
        "\n",
        "pngPath = os.path.join(bratsPath, 'pngImages')\n",
        "pngHGGPath = os.path.join(pngPath, 'HGGpng')\n",
        "pngLGGPath = os.path.join(pngPath, 'LGGpng')\n",
        "DATASET_REAL = os.path.join(pngPath, 'DATASET_R')\n",
        "\n",
        "if not (os.path.isdir(pngPath)):\n",
        "  os.mkdir(pngPath)\n",
        "\n",
        "if not (os.path.isdir(pngHGGPath)):\n",
        "  os.mkdir(pngHGGPath)\n",
        "\n",
        "if not (os.path.isdir(pngLGGPath)):\n",
        "  os.mkdir(pngLGGPath)\n",
        "\n",
        "if not (os.path.isdir(DATASET_REAL)):\n",
        "  os.mkdir(DATASET_REAL)\n",
        "\n",
        "# Convert NIfTi files to PNG images ##############################################\n",
        "# take slices at 60, 70, 80\n",
        "# Brats NIfTI mask files\n",
        "#   label of 0: nothing\n",
        "#   label of 1: non-enhancing tumor core\n",
        "#   label of 2: peritumoral edema\n",
        "#   label of 4: GD-enhancing tumor\n",
        "# Since we are only segmenting the tumor core, we will conver the image into two labels:\n",
        "#   label of 0: nothing\n",
        "#   label of 1: GD-enhancing and non-enhancing tumor \n",
        "print('Converting HGG NIfTI files')\n",
        "for subject in os.listdir(niftiHGGPath):\n",
        "  print(subject)\n",
        "  t1ce_niftiFilePath = os.path.join(niftiHGGPath, subject,subject+'_t1ce.nii.gz')\n",
        "  seg_niftiFilePath = os.path.join(niftiHGGPath, subject,subject+'_seg.nii.gz')\n",
        "  for slice in [60,70,80]:\n",
        "    print(str(slice)+'  t1ce')\n",
        "    b = nib.load(t1ce_niftiFilePath);  m = b.get_fdata()[:,:,slice];\n",
        "    t1ce_pngFilePath = os.path.join(pngHGGPath,subject+'_z'+str(slice)+'_t1ce.png')\n",
        "    imageio.imwrite(t1ce_pngFilePath, m)\n",
        "    print(str(slice)+'  seg')\n",
        "    b = nib.load(seg_niftiFilePath);  m = b.get_fdata()[:,:,slice];  m[m==4] = 1;  m[m==2] = 0\n",
        "    seg_pngFilePath = os.path.join(pngHGGPath,subject+'_z'+str(slice)+'_seg.png')\n",
        "    imageio.imwrite(seg_pngFilePath, m)\n",
        "print('Converting LGG NIfTI files')\n",
        "for subject in os.listdir(niftiLGGPath):\n",
        "  print(subject)\n",
        "  t1ce_niftiFilePath = os.path.join(niftiLGGPath, subject,subject+'_t1ce.nii.gz')\n",
        "  seg_niftiFilePath = os.path.join(niftiLGGPath, subject,subject+'_seg.nii.gz')\n",
        "  for slice in [60,70,80]:\n",
        "    print(str(slice)+'  t1ce')\n",
        "    b = nib.load(t1ce_niftiFilePath);  m = b.get_fdata()[:,:,slice];\n",
        "    t1ce_pngFilePath = os.path.join(pngLGGPath,subject+'_z'+str(slice)+'_t1ce.png')\n",
        "    imageio.imwrite(t1ce_pngFilePath, m)\n",
        "    print(str(slice)+'  seg')\n",
        "    b = nib.load(seg_niftiFilePath);  m = b.get_fdata()[:,:,slice];  m[m==4] = 1;  m[m==2] = 0\n",
        "    seg_pngFilePath = os.path.join(pngLGGPath,subject+'_z'+str(slice)+'_seg.png')\n",
        "    imageio.imwrite(seg_pngFilePath, m)\n",
        "\n",
        "# Select PNG images for DATASET ##################################################\n",
        "i = 0;\n",
        "pngHGGPathList = glob('/content/gdrive/My Drive/Brats19/pngImages/HGGpng/*seg.png')\n",
        "for oldpath in pngHGGPathList:\n",
        "  newpath = os.path.join(DATASET_REAL, str(i)+'_mask.png')\n",
        "  copyfile(oldpath,newpath)\n",
        "  oldpath = oldpath.replace('_seg.png','') + '_t1ce.png'\n",
        "  newpath = os.path.join(DATASET_REAL, str(i)+'.png')\n",
        "  copyfile(oldpath,newpath)\n",
        "  i +=1; print(i)\n",
        "pngLGGPathList = glob('/content/gdrive/My Drive/Brats19/pngImages/LGGpng/*seg.png')\n",
        "for oldpath in pngLGGPathList:\n",
        "  newpath = os.path.join(DATASET_REAL, str(i)+'_mask.png')\n",
        "  copyfile(oldpath,newpath)\n",
        "  oldpath = oldpath.replace('_seg.png','') + '_t1ce.png'\n",
        "  newpath = os.path.join(DATASET_REAL, str(i)+'.png')\n",
        "  copyfile(oldpath,newpath)\n",
        "  i +=1; print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Min0tn8zct9v",
        "colab_type": "text"
      },
      "source": [
        "# **Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S1x8csDd14I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augplot(original, method):\n",
        "  # when ...\n",
        "  #   type = 1    ->   vertical flip\n",
        "  #   type = 2    ->   horizontal flip\n",
        "  #   type = 3    ->   rotate 180 degrees\n",
        "  if method == 1:\n",
        "    augmented = original.transpose(Image.FLIP_TOP_BOTTOM) # flip vert\n",
        "  elif method == 2:\n",
        "    augmented = original.transpose(Image.FLIP_LEFT_RIGHT) # flip hor\n",
        "  elif method == 3:\n",
        "    augmented = original.transpose(Image.ROTATE_180) # rot 180\n",
        "  elif method == 4:\n",
        "    # elastic deformation:\n",
        "    alpha = 700 # Determines deformation intensity\n",
        "    sigma =  24 # Standard deviation of displacements / Elastic coefficient\n",
        "    random_state = np.random.RandomState(None)\n",
        "    shape = original.size \n",
        "    # create normally distributed displacements\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n",
        "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
        "    distored_image = map_coordinates(original, indices, order=1, mode='reflect')\n",
        "    elastic = distored_image.reshape(original.size)\n",
        "    # Convert numpy array to image\n",
        "    augmented = Image.fromarray(elastic) \n",
        "  \n",
        "  return augmented\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, sharex=True, sharey=True, figsize=(20, 15), gridspec_kw={'wspace': 0.025, 'hspace': 0.000010})\n",
        "\n",
        "im1 = mpimg.imread('/content/gdrive/My Drive/Brats19/pngImages/DATASET_R/4.png')\n",
        "im6 = mpimg.imread('/content/gdrive/My Drive/Brats19/pngImages/DATASET_R/4_mask.png')\n",
        "\n",
        "axs[0][0].set_title(\"Original Image\", fontdict={'fontsize': 16})\n",
        "axs[0][0].imshow(im1, cmap='gray')\n",
        "axs[0][0].set_axis_off()\n",
        "axs[1][0].set_title(\"\", fontdict={'fontsize': 16})\n",
        "axs[1][0].imshow(im6, cmap='gray')\n",
        "axs[1][0].set_axis_off()\n",
        "\n",
        "m1 = Image.open('/content/gdrive/My Drive/Brats19/pngImages/DATASET_R/4.png')\n",
        "m2 = Image.open('/content/gdrive/My Drive/Brats19/pngImages/DATASET_R/4_mask.png')\n",
        "\n",
        "out1 = augplot(m1,1)\n",
        "out2 = augplot(m2,1)\n",
        "axs[0][1].set_title(\"Vertical Flip\", fontdict={'fontsize': 16})\n",
        "axs[0][1].imshow(out1, cmap='gray')\n",
        "axs[0][1].set_axis_off()\n",
        "axs[1][1].set_title(\"\", fontdict={'fontsize': 16})\n",
        "axs[1][1].imshow(out2, cmap='gray')\n",
        "axs[1][1].set_axis_off()\n",
        "\n",
        "out1 = augplot(m1,2)\n",
        "out2 = augplot(m2,2)\n",
        "axs[0][2].set_title(\"Horizontal Flip\", fontdict={'fontsize': 16})\n",
        "axs[0][2].imshow(out1, cmap='gray')\n",
        "axs[0][2].set_axis_off()\n",
        "axs[1][2].set_title(\"\", fontdict={'fontsize': 16})\n",
        "axs[1][2].imshow(out2, cmap='gray')\n",
        "axs[1][2].set_axis_off()\n",
        "\n",
        "out1 = augplot(m1,3)\n",
        "out2 = augplot(m2,3)\n",
        "axs[0][3].set_title(\"180deg Rotate\", fontdict={'fontsize': 16})\n",
        "axs[0][3].imshow(out1, cmap='gray')\n",
        "axs[0][3].set_axis_off()\n",
        "axs[1][3].set_title(\"\", fontdict={'fontsize': 16})\n",
        "axs[1][3].imshow(out2, cmap='gray')\n",
        "axs[1][3].set_axis_off()\n",
        "\n",
        "out1 = augplot(m1,4)\n",
        "out2 = augplot(m2,4)\n",
        "axs[0][4].set_title(\"Elastic Distortion\", fontdict={'fontsize': 16})\n",
        "axs[0][4].imshow(out1, cmap='gray')\n",
        "axs[0][4].set_axis_off()\n",
        "axs[1][4].set_title(\"\", fontdict={'fontsize': 16})\n",
        "axs[1][4].imshow(out2, cmap='gray')\n",
        "axs[1][4].set_axis_off()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYRg8Qe2K5lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Agmentation function ##########################################################\n",
        "def augmentation(original, method):\n",
        "  # when ...\n",
        "  #   type = 1    ->   vertical flip\n",
        "  #   type = 2    ->   horizontal flip\n",
        "  #   type = 3    ->   rotate 180 degrees\n",
        "  if method == 1:\n",
        "    augmented = original.transpose(Image.FLIP_TOP_BOTTOM) # flip vert\n",
        "  elif method == 2:\n",
        "    augmented = original.transpose(Image.FLIP_LEFT_RIGHT) # flip hor\n",
        "  elif method == 3:\n",
        "    augmented = original.transpose(Image.ROTATE_180) # rot 180\n",
        "\n",
        "  # elastic deformation:\n",
        "  alpha = 100 # Determines deformation intensity\n",
        "  sigma =  10 # Standard deviation of displacements / Elastic coefficient\n",
        "  random_state = np.random.RandomState(None)\n",
        "  shape = augmented.size \n",
        "  # create normally distributed displacements\n",
        "  dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "  dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "  x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n",
        "  indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
        "  distored_image = map_coordinates(augmented, indices, order=1, mode='reflect')\n",
        "  elastic = distored_image.reshape(augmented.size)\n",
        "  # Convert numpy array to image\n",
        "  augmented = Image.fromarray(elastic) \n",
        "  \n",
        "  return augmented\n",
        "\n",
        "# Define paths and make directories ##############################################\n",
        "bratsPath = '/content/gdrive/My Drive/Brats19'\n",
        "\n",
        "pngPath = os.path.join(bratsPath, 'pngImages')\n",
        "DATASET_REALnFAKE = os.path.join(pngPath, 'DATASET_RnF2') \n",
        "\n",
        "if not (os.path.isdir(pngPath)):\n",
        "  os.mkdir(pngPath)\n",
        "\n",
        "if not (os.path.isdir(DATASET_REALnFAKE)):\n",
        "  os.mkdir(DATASET_REALnFAKE)\n",
        "\n",
        "\n",
        "# Saves augmented data and original data in a new dataset ########################\n",
        "datasetDIR = glob('/content/gdrive/My Drive/Brats19/pngImages/DATASET_R/*_mask.png')\n",
        "\n",
        "types = 1;  i = 1005;\n",
        "for oldpath in datasetDIR:\n",
        "\n",
        "  # for MASK #######################\n",
        "  filename = oldpath.replace('/content/gdrive/My Drive/Brats19/pngImages/DATASET_R/','')\n",
        "  newpath = os.path.join(DATASET_REALnFAKE,filename)\n",
        "  copyfile(oldpath,newpath)\n",
        "  augfilename = str(i) + filename[-9:]\n",
        "  augpath = os.path.join(DATASET_REALnFAKE,augfilename)\n",
        "  orig = Image.open(oldpath)\n",
        "  aug = augmentation(orig, types)\n",
        "  aug.save(augpath)\n",
        "\n",
        "  # for SCAN #######################\n",
        "  oldpath = oldpath.replace('_mask','')\n",
        "  filename = oldpath.replace('/content/gdrive/My Drive/Brats19/pngImages/DATASET_R/','')\n",
        "  newpath = os.path.join(DATASET_REALnFAKE,filename)\n",
        "  copyfile(oldpath,newpath)\n",
        "  augfilename = str(i) + filename[-4:]\n",
        "  augpath = os.path.join(DATASET_REALnFAKE,augfilename)\n",
        "  orig = Image.open(oldpath)\n",
        "  aug = augmentation(orig, types)\n",
        "  aug.save(augpath)\n",
        "\n",
        "  # # cycle through methods\n",
        "  # if types < 3:\n",
        "  #   types +=1;\n",
        "  # elif types == 3: \n",
        "  #   types = 1;\n",
        "\n",
        "  i += 1; print(i)\n",
        "\n",
        "print(len(glob('/content/gdrive/My Drive/Brats19/pngImages/DATASET_R/*.png')))\n",
        "print(len(glob('/content/gdrive/My Drive/Brats19/pngImages/DATASET_RnF2/*.png')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSAaAoYTc1pu",
        "colab_type": "text"
      },
      "source": [
        "# **Unet: Segmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qufvx8yudxHD",
        "colab_type": "text"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Qe3UTYHBvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(4460);\n",
        "np.random.seed(4460)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Computation Details')\n",
        "print(f'\\tDevice Used: ({device})  {torch.cuda.get_device_name(torch.cuda.current_device())}\\n')\n",
        "print('Packages Used Versions:-')\n",
        "print(f'\\tPytorch Version: {torch.__version__}')\n",
        "\n",
        "# Dataset folder used\n",
        "# DATASET_PATH = DATASET_REAL\n",
        "DATASET_PATH = DATASET_REALnFAKE\n",
        "# We would like to perform a train-validation-test split at the ratio of T:V:T = 6:2:2.\n",
        "VAL_SPLIT = 0.2;  TEST_SPLIT = 0.2;\n",
        "# Batch size for training. Limited by GPU memory\n",
        "BATCH_SIZE = 6\n",
        "# Training Epochs\n",
        "epochs = 60\n",
        "\n",
        "# Functions ##############################################################################\n",
        "class TumorDataset(Dataset):\n",
        "    ''' Returns a TumorDataset class object which represents our tumor dataset.\n",
        "        TumorDataset inherits from torch.utils.data.Dataset class.\n",
        "    '''\n",
        "    def __init__(self, root_dir, DEBUG = False):\n",
        "        '''\n",
        "        Constructor for our TumorDataset class.\n",
        "        Parameters:\n",
        "            root_dir(str): Directory with all the images.\n",
        "            DEBUG(bool): To switch to debug mode for image transformation.\n",
        "        Returns: None\n",
        "        '''\n",
        "        self.root_dir = root_dir\n",
        "        # The default transformation is composed of \n",
        "        # 1) a grayscale conversion and 2) a resizing to 512 x 512.\n",
        "        self.default_transformation = transforms.Compose([\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize((512, 512))\n",
        "        ])\n",
        "        self.DEBUG = DEBUG\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        Overridden method from inheritted class to support\n",
        "        indexing of dataset such that datset[I] can be used\n",
        "        to get Ith sample.\n",
        "        Parameters:  index(int): Index of the dataset sample\n",
        "        Return:  sample(dict): Contains the index, image, mask torch.Tensor.\n",
        "                        'index': Index of the image.\n",
        "                        'image': Contains the tumor image torch.Tensor.\n",
        "                        'mask' : Contains the mask image torch.Tensor.\n",
        "        '''\n",
        "        # Find the filenames for the tumor images and masks.\n",
        "        image_name = os.path.join(self.root_dir, str(index) + '.png')\n",
        "        mask_name = os.path.join(self.root_dir, str(index) + '_mask.png')\n",
        "\n",
        "        # Use PIL to open the images and masks.\n",
        "        image = Image.open(image_name)\n",
        "        mask = Image.open(mask_name)\n",
        "\n",
        "        # Apply the default transformations on the images and masks.\n",
        "        image = self.default_transformation(image)\n",
        "        mask = self.default_transformation(mask)\n",
        "\n",
        "        # Convert the images and masks to tensor.\n",
        "        image = TF.to_tensor(image)\n",
        "        mask = TF.to_tensor(mask)\n",
        "\n",
        "        # Construct the images and masks together in the form of a dictionary.\n",
        "        sample = {'index': int(index), 'image': image, 'mask': mask}\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        ''' Overridden method from inheritted class so that\n",
        "        len(self) returns the size of the dataset.\n",
        "        '''\n",
        "        error_msg = 'Part of dataset is missing!\\nNumber of tumor and mask images are not same.'\n",
        "        total_files = len(glob(os.path.join(self.root_dir, '*.png')))\n",
        "\n",
        "        # Sanity check: the number of files shall be even since tumor images and masks are in pairs.\n",
        "        assert total_files % 2 == 0, error_msg + ' total files: %s' % (total_files)\n",
        "        \n",
        "        # Return how many image-mask pairs we have.\n",
        "        return total_files // 2\n",
        "    \n",
        "def get_indices(length, val_split, test_split):\n",
        "    ''' Gets the Training & Testing data indices for the dataset.\n",
        "        Stores the indices and returns them back when the same dataset is used.\n",
        "    Inputs:\n",
        "        length(int): Length of the dataset used.\n",
        "        val_split: the portion (0 to 1) of data used for validation.\n",
        "        test_split: the portion (0 to 1) of data used for testing.\n",
        "    Parameters:\n",
        "        train_indices(list): Array of indices used for training purpose.\n",
        "        validation_indices(list): Array of indices used for validation purpose.\n",
        "        test_indices(list): Array of indices used for testing purpose.\n",
        "    '''\n",
        "    data = dict()\n",
        "    indices = list(range(length))\n",
        "    np.random.shuffle(indices)\n",
        "    split1 = int(np.floor(test_split * len(tumor_dataset)))\n",
        "    split2 = split1 + int(np.floor(val_split * len(tumor_dataset)))\n",
        "    train_indices, validation_indices, test_indices = indices[split2:], indices[split1:split2], indices[:split1]\n",
        "    return train_indices, validation_indices, test_indices\n",
        "\n",
        "# Split ##############################################################################\n",
        "tumor_dataset = TumorDataset(DATASET_PATH)\n",
        "\n",
        "train_indices, validation_indices, test_indices = get_indices(len(tumor_dataset), val_split = VAL_SPLIT, test_split = TEST_SPLIT)\n",
        "train_sampler, validation_sampler, test_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(validation_indices), SubsetRandomSampler(test_indices)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(tumor_dataset, BATCH_SIZE, sampler = train_sampler)\n",
        "validationloader = torch.utils.data.DataLoader(tumor_dataset, 1, sampler = validation_sampler)\n",
        "testloader = torch.utils.data.DataLoader(tumor_dataset, 1, sampler = test_sampler)\n",
        "\n",
        "print('Number of files in the train set: %s \\nNumber of files in the validation set: %s \\nNumber of files in the test set: %s' \\\n",
        "      % (len(train_indices), len(validation_indices), len(test_indices)))\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "\n",
        "\n",
        "# UNET model ##############################################################################\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class up_conv(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(up_conv, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor = 2),\n",
        "            nn.Conv2d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "    \n",
        "class U_Net(nn.Module):\n",
        "    ''' This is the Pytorch version of U-Net Architecture.\n",
        "        The input and output of this network is of the same shape.\n",
        "    Input Size of Network - (1,512,512). \n",
        "        Note that the input size here is just for our dataset in this notebook, but if you use this network for other \n",
        "        projects, any input size that is a multiple of 2 ** 5 will work.\n",
        "    Output Size of Network - (1,512,512)\n",
        "        Shape Format :  (Channel, Width, Height)\n",
        "    '''\n",
        "    def __init__(self, img_ch = 1, output_ch = 1, first_layer_numKernel = 16):\n",
        "        ''' Constructor for UNet class.\n",
        "        Parameters:\n",
        "            img_ch(int): Input channels for the network. Default: 1\n",
        "            output_ch(int): Output channels for the final network. Default: 1\n",
        "            first_layer_numKernel(int): Number of kernels uses in the first layer of our unet.\n",
        "        '''\n",
        "        super(U_Net, self).__init__()\n",
        "        \n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.Conv1 = conv_block(ch_in = img_ch, ch_out = first_layer_numKernel)\n",
        "        self.Conv2 = conv_block(ch_in = first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n",
        "        self.Conv3 = conv_block(ch_in = 2 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n",
        "        self.Conv4 = conv_block(ch_in = 4 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n",
        "        self.Conv5 = conv_block(ch_in = 8 * first_layer_numKernel, ch_out = 16 * first_layer_numKernel)\n",
        "\n",
        "        self.Up5 = up_conv(ch_in = 16 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n",
        "        self.Up_conv5 = conv_block(ch_in = 16 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in = 8 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n",
        "        self.Up_conv4 = conv_block(ch_in = 8 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n",
        "        \n",
        "        self.Up3 = up_conv(ch_in = 4 * first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n",
        "        self.Up_conv3 = conv_block(ch_in = 4 * first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n",
        "        \n",
        "        self.Up2 = up_conv(ch_in = 2 * first_layer_numKernel, ch_out = first_layer_numKernel)\n",
        "        self.Up_conv2 = conv_block(ch_in = 2 * first_layer_numKernel, ch_out = first_layer_numKernel)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(first_layer_numKernel, output_ch, kernel_size = 1, stride = 1, padding = 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Method for forward propagation in the network.\n",
        "        Parameters:\n",
        "            x(torch.Tensor): Input for the network of size (1, 512, 512).\n",
        "        Returns:\n",
        "            output(torch.Tensor): Output after the forward propagation of network on the input.\n",
        "        '''\n",
        "        # encoding path\n",
        "        x1 = self.Conv1(x)\n",
        "\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.Conv2(x2)\n",
        "        \n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.Conv3(x3)\n",
        "        \n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.Conv4(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.Conv5(x5)\n",
        "        \n",
        "        # decoding + concat path\n",
        "        d5 = self.Up5(x5)\n",
        "        d5 = torch.cat((x4, d5), dim = 1)\n",
        "        \n",
        "        d5 = self.Up_conv5(d5)\n",
        "        \n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((x3, d4), dim = 1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((x2, d3), dim = 1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((x1, d2), dim = 1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "        output = F.sigmoid(d1)\n",
        "\n",
        "        return output\n",
        "    \n",
        "# Dice Coeff ##############################################################################\n",
        "def dice_coefficient(predicted, target):\n",
        "    ''' Calculates the Sørensen–Dice Coefficient for a single sample.\n",
        "    Parameters:\n",
        "        predicted(numpy.ndarray): Predicted single output of the network.\n",
        "                                Shape - (Channel, Height, Width)\n",
        "        target(numpy.ndarray): Actual required single output for the network\n",
        "                                Shape - (Channel, Height, Width)\n",
        "    Returns:\n",
        "        coefficient(float): Dice coefficient for the input sample.\n",
        "                                    1 represents highest similarity and\n",
        "                                    0 represents lowest similarity.\n",
        "    '''\n",
        "    # The smooth term is used to prevent division by zero.\n",
        "    smooth = 1\n",
        "    product = np.multiply(predicted, target)\n",
        "    intersection = np.sum(product)\n",
        "    coefficient = (2 * intersection + smooth) / (np.sum(predicted) + np.sum(target) + smooth)\n",
        "    return coefficient\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBzgjjhtdl2O",
        "colab_type": "text"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSGlCHs4dmFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unet_model = None\n",
        "unet_classifier = None\n",
        "criterion = nn.BCELoss()\n",
        "learning_rate = 0.0001\n",
        "\n",
        "#### If you want to see the training trend within each epoch, you can change mini_batch to a positive integer \n",
        "#### that is no larger than the number of batches per epoch.\n",
        "mini_batch = False\n",
        "\n",
        "# Define where to save the model parameters.\n",
        "model_save_path = './saved_models/'\n",
        "os.makedirs(model_save_path, exist_ok = True)\n",
        "\n",
        "# New model is created.\n",
        "unet_model = U_Net().to(device)\n",
        "\n",
        "\n",
        "# Training session history data.\n",
        "history = {'train_loss': list(), 'validation_loss': list()}\n",
        "\n",
        "# For save best feature. Initial loss taken a very high value.\n",
        "last_score = 0\n",
        "\n",
        "# Optimizer used for training process. Adam Optimizer.\n",
        "optimizer = optim.Adam(unet_model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Reducing LR on plateau feature to improve training.\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2, verbose = True)\n",
        "\n",
        "print('Starting Training Process')\n",
        "\n",
        "assert validationloader.batch_size == 1\n",
        "\n",
        "# Epoch Loop\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    #################################### Train ####################################################\n",
        "    unet_model.train()\n",
        "    start_time = time()\n",
        "    # Training a single epoch\n",
        "    train_epoch_loss, train_batch_loss, batch_iteration = 0, 0, 0\n",
        "    validation_score, validation_loss = 0, 0\n",
        "\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        # Keeping track how many iteration is happening.\n",
        "        batch_iteration += 1\n",
        "        # Loading data to device used.\n",
        "        image = data['image'].to(device)\n",
        "        mask = data['mask'].to(device)\n",
        "        # Clearing gradients of optimizer.\n",
        "        optimizer.zero_grad()\n",
        "        # Calculation predicted output using forward pass.\n",
        "        output = unet_model(image)\n",
        "        # Calculating the loss value.\n",
        "        loss_value = criterion(output, mask)\n",
        "        # Computing the gradients.\n",
        "        loss_value.backward()\n",
        "        # Optimizing the network parameters.\n",
        "        optimizer.step()\n",
        "        # Updating the running training loss\n",
        "        train_epoch_loss += loss_value.item()\n",
        "        train_batch_loss += loss_value.item()\n",
        "\n",
        "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
        "        if mini_batch:\n",
        "            if (batch + 1) % mini_batch == 0:\n",
        "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
        "                print(\n",
        "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
        "                train_batch_loss = 0\n",
        "\n",
        "\n",
        "    train_epoch_loss = train_epoch_loss / (batch_iteration * trainloader.batch_size)\n",
        "    #train_epoch_loss = 0\n",
        "\n",
        "    \n",
        "    ################################### Validation ##################################################\n",
        "    unet_model.eval()\n",
        "    # To get data in loops.\n",
        "    batch_iteration = 0\n",
        "\n",
        "    for batch, data in enumerate(validationloader):\n",
        "        # Keeping track how many iteration is happening.\n",
        "        batch_iteration += 1\n",
        "        # Data prepared to be given as input to model.\n",
        "        image = data['image'].to(device)\n",
        "        mask = data['mask'].to(device)\n",
        "\n",
        "        # Predicted output from the input sample.\n",
        "        mask_prediction = unet_model(image)\n",
        "        \n",
        "        # comput validation loss\n",
        "        loss_value = criterion(mask_prediction, mask)\n",
        "        validation_loss += loss_value.item()\n",
        "        \n",
        "        # Threshold elimination.\n",
        "        mask_prediction = (mask_prediction > 0.5)\n",
        "        mask_prediction = mask_prediction.cpu().numpy()\n",
        "        mask = mask.cpu().numpy()\n",
        "\n",
        "        mask = np.resize(mask, (1, 512, 512))\n",
        "        mask_prediction = np.resize(mask_prediction, (1, 512, 512))\n",
        "        # Calculate the dice score for original and predicted image mask.\n",
        "        validation_score += dice_coefficient(mask_prediction, mask)\n",
        "\n",
        "    # Calculating the mean score for the whole validation dataset.    \n",
        "    unet_val = validation_score / batch_iteration\n",
        "    validation_loss = validation_loss / batch_iteration\n",
        "    \n",
        "    # Collecting all epoch loss values for future visualization.\n",
        "    history['train_loss'].append(train_epoch_loss)\n",
        "    history['validation_loss'].append(validation_loss)\n",
        "    \n",
        "    # Reduce LR On Plateau\n",
        "    scheduler.step(validation_loss)\n",
        "\n",
        "    time_taken = time() - start_time\n",
        "    \n",
        "    # Training Logs printed.\n",
        "    print(f'Epoch: {epoch + 1:3d},  ', end = '')\n",
        "    print(f'train Loss: {train_epoch_loss:.5f},  ', end = '')\n",
        "    print(f'validation Loss: {validation_loss:.5f},  ', end = '')\n",
        "    print(f'validation score: {unet_val:.5f},  ', end = '')\n",
        "\n",
        "    for pg in optimizer.param_groups:\n",
        "        print('current lr: ', pg['lr'], ', ', end = '')\n",
        "    print(f'Time: {time_taken:.2f} s', end = '')\n",
        "\n",
        "    # Save the model every epoch.\n",
        "    current_epoch_model_save_path = os.path.join(model_save_path, 'Basic_Unet_epoch_%s.pth' % (str(epoch).zfill(3)))\n",
        "    torch.save(unet_model.state_dict(), current_epoch_model_save_path)\n",
        "    \n",
        "    # Save the best model (determined by validation score) and give it a unique name.\n",
        "    best_model_path = os.path.join(model_save_path, 'Basic_Unet_best_model.pth')\n",
        "    if  last_score < unet_val:\n",
        "        torch.save(unet_model.state_dict(), best_model_path)\n",
        "        last_score = unet_val\n",
        "        print(f'\\tBest model saved at score: {unet_val:.5f}')\n",
        "    else:\n",
        "        print()\n",
        "\n",
        "print(f'Training Finished after {epochs} epoches')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrrFSnhfd6AZ",
        "colab_type": "text"
      },
      "source": [
        "### **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gc2K6Jrd6Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training loss graph ##############################################################################\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title('Loss Over Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "train_curve = plt.plot(history['train_loss'], marker = 'o', label = 'Train loss')\n",
        "validation_curve = plt.plot(history['validation_loss'], marker = 'o', label = 'Validation loss')\n",
        "plt.legend(fontsize = 15)\n",
        "plt.show()\n",
        "\n",
        "# test model ##############################################################################\n",
        "# Load the unet model at its prime (when it performed the best on the validation set).\n",
        "state_dict = torch.load(os.path.join(model_save_path, 'Basic_Unet_best_model.pth'))\n",
        "unet_model.load_state_dict(state_dict)\n",
        "\n",
        "# Testing process on test data.\n",
        "unet_model.eval()\n",
        "# Getting test data indices for dataloading\n",
        "test_data_indexes = test_indices\n",
        "# Total testing data used.\n",
        "data_length = len(test_data_indexes)\n",
        "# Score after testing on dataset.\n",
        "mean_test_score = 0\n",
        "\n",
        "for batch, data in enumerate(testloader):\n",
        "    # Data prepared to be given as input to model.\n",
        "    image = data['image'].to(device)\n",
        "    mask = data['mask']\n",
        "\n",
        "    # Predicted output from the input sample.\n",
        "    mask_prediction = unet_model(image).cpu()\n",
        "    # Threshold elimination.\n",
        "    mask_prediction = (mask_prediction > 0.5)\n",
        "    mask_prediction = mask_prediction.numpy()\n",
        "\n",
        "    mask = np.resize(mask, (1, 512, 512))\n",
        "    mask_prediction = np.resize(mask_prediction, (1, 512, 512))\n",
        "\n",
        "    # Calculating the dice score for original and predicted mask.\n",
        "    mean_test_score += dice_coefficient(mask_prediction, mask)\n",
        "\n",
        "# Calculating the mean score for the whole test dataset.\n",
        "unet_score = mean_test_score / data_length\n",
        "# Putting the model back to training mode.\n",
        "print(f'\\nDice Score {unet_score}\\n')\n",
        "\n",
        "# visualize prediction ##############################################################################\n",
        "\n",
        "def result(image, mask, output, title, transparency = 0.38, save_path = None):\n",
        "    '''\n",
        "    Plots a 2x3 plot with comparisons of output and original image.\n",
        "    Works best with Jupyter Notebook/Lab.\n",
        "    Parameters:\n",
        "        image(numpy.ndarray): Array containing the original image of MRI scan.\n",
        "        mask(numpy.ndarray): Array containing the original mask of tumor.\n",
        "        output(numpy.ndarray): Model predicted mask from input image.\n",
        "        title(str): Title of the plot to be used.\n",
        "        transparency(float): Transparency level of mask on images.\n",
        "                             Default: 0.38\n",
        "        save_path(str): Saves the plot to the location specified.\n",
        "                        Does nothing if None. \n",
        "                        Default: None\n",
        "    Return:\n",
        "        None\n",
        "    '''\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(\n",
        "        20, 15), gridspec_kw={'wspace': 0.025, 'hspace': 0.010})\n",
        "    fig.suptitle(title, x=0.5, y=0.92, fontsize=20)\n",
        "\n",
        "    axs[0][0].set_title(\"Original Mask\", fontdict={'fontsize': 16})\n",
        "    axs[0][0].imshow(mask, cmap='gray')\n",
        "    axs[0][0].set_axis_off()\n",
        "\n",
        "    axs[0][1].set_title(\"Predicted Mask\", fontdict={'fontsize': 16})\n",
        "    axs[0][1].imshow(output, cmap='gray')\n",
        "    axs[0][1].set_axis_off()\n",
        "\n",
        "    mask_diff = np.abs(np.subtract(mask, output))\n",
        "    axs[0][2].set_title(\"Mask Difference\", fontdict={'fontsize': 16})\n",
        "    axs[0][2].imshow(mask_diff, cmap='gray')\n",
        "    axs[0][2].set_axis_off()\n",
        "\n",
        "    seg_output = mask*transparency\n",
        "    seg_image = np.add(image, seg_output)/2\n",
        "    axs[1][0].set_title(\"Original Segmentation\", fontdict={'fontsize': 16})\n",
        "    axs[1][0].imshow(seg_image, cmap='gray')\n",
        "    axs[1][0].set_axis_off()\n",
        "\n",
        "    seg_output = output*transparency\n",
        "    seg_image = np.add(image, seg_output)/2\n",
        "    axs[1][1].set_title(\"Predicted Segmentation\", fontdict={'fontsize': 16})\n",
        "    axs[1][1].imshow(seg_image, cmap='gray')\n",
        "    axs[1][1].set_axis_off()\n",
        "\n",
        "    axs[1][2].set_title(\"Original Input Image\", fontdict={'fontsize': 16})\n",
        "    axs[1][2].imshow(image, cmap='gray')\n",
        "    axs[1][2].set_axis_off()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi = 90, bbox_inches = 'tight')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "for example_index in range(10):\n",
        "    # The purpose of image_index is to make sure we truly pick from the test set.\n",
        "    image_index = test_indices[example_index]\n",
        "    sample = tumor_dataset[image_index]\n",
        "    threshold = 0.5\n",
        "\n",
        "    unet_model.eval()\n",
        "    image = sample['image'].numpy()\n",
        "    mask = sample['mask'].numpy()\n",
        "\n",
        "    image_tensor = torch.Tensor(image)\n",
        "    image_tensor = image_tensor.view((-1, 1, 512, 512)).to(device)\n",
        "    output = unet_model(image_tensor).detach().cpu()\n",
        "    output = (output > threshold)\n",
        "    output = output.numpy()\n",
        "\n",
        "    # image(numpy.ndarray): 512x512 Original brain scanned image.\n",
        "    image = np.resize(image, (512, 512))\n",
        "    # mask(numpy.ndarray): 512x512 Original mask of scanned image.\n",
        "    mask = np.resize(mask, (512, 512))\n",
        "    # output(numpy.ndarray): 512x512 Generated mask of scanned image.\n",
        "    output = np.resize(output, (512, 512))\n",
        "    # score(float): Sørensen–Dice Coefficient for mask and output. Calculates how similar are the two images.\n",
        "    d_score = dice_coefficient(output, mask)\n",
        "\n",
        "    title = f'Name: {image_index}.png   Dice Score: {d_score:.5f}'\n",
        "    # save_path = os.path.join('images',f'{d_score:.5f}_{image_index}.png')\n",
        "    result(image, mask, output, title, save_path = None)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}